# mnist
Activation function ï¼šRelu, Tanh, Logistic

Net : 3 hidden + 1 out layers , full-connected

Best accuracy on test data: 97%

########################### Relu Tuning log ####### Xavier initialization #####
# order-h1---h2--h3--batch_size---learning_rate--reg---iteration---loss---test_acc 
#  01--100-100-100----200--------1e-3-----------0.005----60000----0.88----0.87
        
#  02--200-300-100----300--------1e-3-----------0.005----60000----1.38----0.77
        
#  03--200-300-100----300--------5e-3-----------0.001-----10000---1.25----0.7  
        
#  04--200-300-100----300--------8e-3-----------0.0001----10000---0.31----0.93
        
#  05--300-300-200----300--------9e-3-----------0.0001----10000---0.48----0.91
        
#  06--300-300-200----300--------1e-2-----------0.0005----10000---0.70----0.88
        
#  07--200-300-100----300--------9e-3-----------0.0001----10000---0.25----0.92

#  08--200-300-100----300--------1e-2-----------0.0001----10000---0.24----0.91

#  09--200-300-100----300--------1e-2(0.95)-----0.0001----10000---0.41----0.89

#  10--200-300-100----300--------2e-2(0.95)-----0.0001----10000---0.54----0.89
        
#  11--200-300-100----300-------1.5e-2(0.95)----0.0001----10000---0.55----0.81

#  12--200-300-100----300--------8e-3(0.95)-----0.0001----30000---0.50----0.89     

#  13--200-300-100----300--------8e-3(1)--------0.0001----30000---0.11----0.96
        
#  14--200-300-100----300--------9e-3(1)--------0.0001----30000---0.11----0.965
        
#  15--200-300-100----300--------9e-3(1)--------0.0003----30000---0.18----0.962
        
#  16--200-300-200----300--------9e-3(1)--------0.0003----30000---0.16----0.954
        
#  16--200-300-50-----300--------9e-3(1)--------0.0001----30000---0.09----0.966
        
#  16--200-200-50-----300--------9e-3(1)--------0.0001----30000---0.12----0.970
 
#  17--200-100-50-----300--------9e-3(1)--------0.0001----30000---0.13----0.959 
        
#  18--200-200-50-----300--------9e-3(1)--------0.0000----30000---0.09----0.9658
###############################################################################
